# ðŸ§  Project: Transformers & RNN with wandb Tracking

This project explores and compares two primary architectures for NLP tasks:

- **RNN (SEQ2SEQ)**: To capture sequential dependencies in textual data.
- **Transformers**: To leverage complex contextual dependencies using multi-head attention mechanisms.

Experiment tracking is powered by **Weights & Biases (wandb)**, providing metrics visualization, hyperparameter history, and centralized performance tracking. The goal is to evaluate the ability of both approaches to solve a specific task while identifying their strengths and limitations.

## ðŸ“š References

- **Original Transformers Paper**: [Attention Is All You Need (Vaswani et al.)](https://arxiv.org/abs/1706.03762)
- **RNNs (LSTM, GRU)**:
  - [Hochreiter & Schmidhuber (1997)](https://www.bioinf.jku.at/publications/older/2604.pdf) - Long Short-Term Memory (LSTM).
  - [Cho et al. (2014)](https://arxiv.org/pdf/1406.1078.pdf) - Gated Recurrent Units (GRU).
- **Weights & Biases (wandb)**: [Documentation](https://docs.wandb.ai/)
- **PyTorch**: [Official Website](https://pytorch.org/)

