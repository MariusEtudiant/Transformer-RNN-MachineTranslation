{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66fc8ef3-b5d5-4569-b13e-a265ba066dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 -m venv pytorch-env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a8647a-9299-48ad-9926-a14cbc5847c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytorch-env\\Scripts\\activate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf55992-ae15-4b7d-b81b-6811c7ff3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: notebook in c:\\users\\marius\\anaconda3\\lib\\site-packages (7.2.2)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\marius\\anaconda3\\lib\\site-packages (6.28.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from notebook) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from notebook) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from notebook) (4.2.5)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from notebook) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from notebook) (6.4.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (8.27.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (25.1.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marius\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (305.1)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (21.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.1.4)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.16.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.14.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (2.0.10)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (0.27.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (2.2.0)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (75.1.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\marius\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.27.1->notebook) (2024.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\marius\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\marius\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (0.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.10.6)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\marius\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.2.3)\n",
      "Requirement already satisfied: executing in c:\\users\\marius\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\marius\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\marius\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n",
      "Requirement already satisfied: webencodings in c:\\users\\marius\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.5.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (2.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\marius\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install notebook ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c3ef5b-caed-4fa9-b685-255ea5e816b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\marius\\anaconda3\\lib\\site-packages (2.5.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\marius\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\marius\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marius\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marius\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "#installations des librairies\n",
    "!pip install torch --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa9ad03-2760-4dd5-bd30-623b56e6b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\marius\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58df15c-1f53-4558-857a-4b02b2645715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\marius\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "979167b2-d2a2-4425-ab14-6a506f5e8ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\marius\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0f2bcad3-2796-43dd-b25c-fb8b7f4f1b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\marius\\anaconda3\\lib\\site-packages (0.19.2)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marius\\anaconda3\\lib\\site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\marius\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\marius\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: marius-casamian (marius-casamian-sophia-antipolis). Use `wandb login --relogin` to force relogin\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2c5f7be-da93-4e0d-ad4c-4133574e51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des librairies pour le RNN\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b94c463a-829d-4bda-9da0-9c0fd7272e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des librairies pour le RNN  + Transformer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import wandb\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "220f64cf-5753-49ce-b920-49989289743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARTIE 1 : CHAR-LEVEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f7882ca0-249e-4f48-93f2-fa0fb51b2331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  en  \\\n",
      "0  Changing Lives | Changing Society | How It Wor...   \n",
      "1                                           Site map   \n",
      "2                                           Feedback   \n",
      "3                                            Credits   \n",
      "4                                           Français   \n",
      "\n",
      "                                                  fr  \n",
      "0  Il a transformé notre vie | Il a transformé la...  \n",
      "1                                       Plan du site  \n",
      "2                                        Rétroaction  \n",
      "3                                            Crédits  \n",
      "4                                            English  \n"
     ]
    }
   ],
   "source": [
    "# Lecture du dataset\n",
    "dataset_path = \"Desktop/Projet_Methodes_Apprentissages/inputs/en-fr.csv\"  \n",
    "df = pd.read_csv(dataset_path, usecols=[\"en\", \"fr\"], nrows=100000)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b244998-92ee-406d-9bb8-516ebb53333d",
   "metadata": {},
   "source": [
    "#### Tokenizer Simple caractère par caractère"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b28a75c-4037-4541-9610-2ad1aa350092",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer qui coupe chaque phrase caractère par caractère.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialisation des tokens spéciaux\n",
    "        self.char2id = {\"<bos>\": 0, \"<eos>\": 1}\n",
    "        self.id2char = {0: \"<bos>\", 1: \"<eos>\"}\n",
    "        \n",
    "    def fit(self, sentences):\n",
    "        \"\"\"\n",
    "        Construit le vocabulaire à partir d'une liste de phrases\n",
    "        \"\"\"\n",
    "        for sentence in sentences:\n",
    "            for char in sentence:\n",
    "                if char not in self.char2id:\n",
    "                    idx = len(self.char2id)\n",
    "                    self.char2id[char] = idx\n",
    "                    self.id2char[idx] = char\n",
    "        \n",
    "    def encode(self, sentence):\n",
    "        \"\"\"\n",
    "        Convertit une phrase en liste d'ID.\n",
    "        \"\"\"\n",
    "        # On ignore ici la casse \n",
    "        encoded = [self.char2id[\"<bos>\"]]\n",
    "        for char in sentence:\n",
    "            if char in self.char2id:\n",
    "                encoded.append(self.char2id[char])\n",
    "        encoded.append(self.char2id[\"<eos>\"])\n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Convertit une liste d'ID en phrase.\n",
    "        \"\"\"\n",
    "        #on ignore les tokens et on reconstruit les phrases\n",
    "        chars = []\n",
    "        for idx in ids:\n",
    "            if idx not in (self.char2id[\"<bos>\"], self.char2id[\"<eos>\"]):\n",
    "                chars.append(self.id2char[idx])\n",
    "        return \"\".join(chars)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.char2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45e1b6-280a-4282-a081-84d157308d26",
   "metadata": {},
   "source": [
    "#### Dataset de paires (en - fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bd67388-4ae5-4ba2-a209-93da4733dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset qui gère des paires (source, cible), \n",
    "    utilise un tokenizer (ici char-level) et tronque au besoin.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, tokenizer, max_len=50):\n",
    "        \"\"\"\n",
    "        pairs: liste de tuples (phrase_source, phrase_cible)\n",
    "        tokenizer: instance de SimpleTokenizer\n",
    "        max_len: longueur max pour tronquer\n",
    "        \"\"\"\n",
    "        self.pairs = pairs\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        #tronque pour ne pas exploser la mémoire\n",
    "        src = src[:self.max_len]\n",
    "        tgt = tgt[:self.max_len]\n",
    "\n",
    "        #tokenization et mapping vers ID\n",
    "        src_ids = self.tokenizer.encode(src)\n",
    "        tgt_ids = self.tokenizer.encode(tgt)\n",
    "\n",
    "        #conversion en tenseurs\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor(tgt_ids, dtype=torch.long)\n",
    "\n",
    "        return src_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951337b1-fa47-4eaf-9ba4-2c2f66b90c19",
   "metadata": {},
   "source": [
    "#### Seq2Seq RNN simple "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "669eba39-d1c1-404b-81f3-e3182301a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Modèle de traduction simple RNN.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        #embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        #RNN simple\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        #projection linéaire des états cachés vers le vocab\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        On réalise un \"teacher forcing\" direct dans le code d’entraînement\n",
    "        \"\"\"\n",
    "        #encode phrase source\n",
    "        emb_src = self.embedding(src)   # [batch_size, src_len, embed_dim]\n",
    "        _, hidden = self.rnn(emb_src)   # hidden [1, batch_size, hidden_dim]\n",
    "        \n",
    "        #decoder séquence cible\n",
    "        emb_tgt = self.embedding(tgt)   # [batch_size, tgt_len, embed_dim]\n",
    "        outputs, _ = self.rnn(emb_tgt, hidden)\n",
    "        \n",
    "        #projection vers le vocab\n",
    "        logits = self.fc(outputs)       # [batch_size, tgt_len, vocab_size]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b6ce4-57b5-4130-8e68-9413b6a61ad6",
   "metadata": {},
   "source": [
    "#### Entraînement sur une époque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "77d67831-7888-4ee6-9efc-e15ad3c1c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    \n",
    "    for (src, tgt) in dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        #teacher forcing\n",
    "        # On sépare la cible en (tgt_input, tgt_target)\n",
    "        tgt_input = tgt[:, :-1]   # tout sauf le dernier token\n",
    "        tgt_target = tgt[:, 1:]   # tout sauf le premier token\n",
    "        \n",
    "        #passage modèle\n",
    "        logits = model(src, tgt_input)\n",
    "        \n",
    "        #reshape pour la cross-entropy\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "        logits_2d = logits.reshape(-1, vocab_size)     # [batch_size * seq_len, vocab_size]\n",
    "        targets_1d = tgt_target.reshape(-1)            # [batch_size * seq_len]\n",
    "        \n",
    "        loss = criterion(logits_2d, targets_1d)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        #calcul de la top-1 accuracy\n",
    "        _, preds_top1 = torch.max(logits_2d, dim=1)  # [N]\n",
    "        correct_top1 += (preds_top1 == targets_1d).sum().item()\n",
    "        \n",
    "        #calcul de la top-5 accuracy\n",
    "        top5_vals, top5_idxs = torch.topk(logits_2d, k=5, dim=1) \n",
    "        #on compare la cible à ces 5 IDs\n",
    "        targets_1d_2col = targets_1d.unsqueeze(1)                 \n",
    "        match_matrix = (top5_idxs == targets_1d_2col)             \n",
    "        correct_top5 += match_matrix.any(dim=1).sum().item()\n",
    "        \n",
    "        total_samples += logits_2d.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    top1_acc = correct_top1 / total_samples\n",
    "    top5_acc = correct_top5 / total_samples\n",
    "    \n",
    "    return avg_loss, top1_acc, top5_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d89bc-8722-4766-964c-b29219e5fb6c",
   "metadata": {},
   "source": [
    "#### entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f835329-796f-447d-b0d4-fda37181ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, batch_size=2, epochs=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        avg_loss, top1, top5 = train_one_epoch(model, dataloader, optimizer, device)\n",
    "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Top1: {top1:.4f}, Top5: {top5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609dbbf-9c81-419b-8788-32f33b7c4978",
   "metadata": {},
   "source": [
    "#### Fonction de traduction que l'on modifiera plus tard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70ed98fb-889b-4b67-bcc7-d355ef09069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence_debug(model, source_sentence, tokenizer, max_len=50, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Fonction de traduction simple, en affichant \n",
    "    les probabilités de chaque token généré.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode la phrase source\n",
    "    src_ids = tokenizer.encode(source_sentence)\n",
    "    src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Passage encodeur\n",
    "        emb_src = model.embedding(src_tensor)\n",
    "        _, hidden = model.rnn(emb_src)\n",
    "    \n",
    "    generated_ids = []\n",
    "    #<bos>\n",
    "    current_token = torch.tensor([[tokenizer.char2id[\"<bos>\"]]], dtype=torch.long, device=device)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            emb_tgt = model.embedding(current_token)\n",
    "            output, hidden = model.rnn(emb_tgt, hidden)\n",
    "            logits = model.fc(output.squeeze(1))  # shape: [1, vocab_size]\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            #token aléatoire pondéré par probs\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            print(f\"Token généré : {next_token_id}, Probabilité : {probs[0, next_token_id]:.4f}\")\n",
    "            \n",
    "            if next_token_id == tokenizer.char2id[\"<eos>\"]:\n",
    "                break\n",
    "            \n",
    "            generated_ids.append(next_token_id)\n",
    "            current_token = torch.tensor([[next_token_id]], dtype=torch.long, device=device)\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539a213-e0f7-4dea-934b-6fc540fe3c83",
   "metadata": {},
   "source": [
    "#### Pipeline d'éxécution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb83e3ee-b2f4-4bbf-8793-ed72e2ec1de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appareil utilisé : cuda\n",
      "Nombre de paires chargées : 100000\n",
      "exemple voc généré: {'<bos>': 0, '<eos>': 1}\n",
      "Taille du vocabulaire : 203\n",
      "Source tensor: tensor([0, 2, 3, 4, 5, 6, 7, 5, 6, 8, 9, 1])\n",
      "Target tensor: tensor([ 0, 21, 26,  8,  4,  8, 17, 23,  4,  5, 12,  1])\n",
      "Source (decoded): Changing L\n",
      "Target (decoded): Il a trans\n",
      "Seq2SeqRNN(\n",
      "  (embedding): Embedding(203, 128)\n",
      "  (rnn): RNN(128, 256, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=203, bias=True)\n",
      ")\n",
      "Epoch 1, Loss: 1.7752, Top1: 0.5120, Top5: 0.7996\n",
      "Epoch 2, Loss: 1.7926, Top1: 0.5076, Top5: 0.8005\n",
      "Epoch 3, Loss: 1.9435, Top1: 0.4673, Top5: 0.7759\n",
      "Epoch 4, Loss: 2.0949, Top1: 0.4226, Top5: 0.7525\n",
      "Epoch 5, Loss: 2.1759, Top1: 0.3990, Top5: 0.7388\n",
      "Epoch 6, Loss: 2.2226, Top1: 0.3872, Top5: 0.7311\n",
      "Epoch 7, Loss: 2.2383, Top1: 0.3836, Top5: 0.7284\n",
      "Epoch 8, Loss: 2.2842, Top1: 0.3682, Top5: 0.7217\n",
      "Epoch 9, Loss: 2.2818, Top1: 0.3690, Top5: 0.7226\n",
      "Epoch 10, Loss: 2.2934, Top1: 0.3643, Top5: 0.7211\n",
      "Token généré : 9, Probabilité : 0.1826\n",
      "Token généré : 11, Probabilité : 0.7875\n",
      "Token généré : 8, Probabilité : 0.6673\n",
      "Token généré : 9, Probabilité : 0.0034\n",
      "Token généré : 11, Probabilité : 0.6276\n",
      "Token généré : 12, Probabilité : 0.3875\n",
      "Token généré : 8, Probabilité : 0.8797\n",
      "Token généré : 76, Probabilité : 0.0013\n",
      "Token généré : 69, Probabilité : 0.1814\n",
      "Token généré : 1, Probabilité : 0.0255\n",
      "\n",
      "Traduction de 'hello' = 'Le Les 86'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Appareil utilisé : {device}\")\n",
    "    \n",
    "    english_sentences = df[\"en\"].astype(str).tolist()\n",
    "    french_sentences = df[\"fr\"].astype(str).tolist()\n",
    "    # Création des paires\n",
    "    pairs = list(zip(english_sentences, french_sentences))\n",
    "    print(f\"Nombre de paires chargées : {len(pairs)}\")\n",
    "    \n",
    "    # Construction du tokenizer char-level\n",
    "    tokenizer = SimpleTokenizer()\n",
    "    print(f\"exemple voc généré: {tokenizer.char2id}\")\n",
    "    tokenizer.fit(english_sentences + french_sentences)\n",
    "    print(f\"Taille du vocabulaire : {tokenizer.vocab_size()}\")\n",
    "    \n",
    "    filtered_pairs = [(src, tgt) for src, tgt in pairs if len(src) > 3 and len(tgt) > 3]\n",
    "    dataset = SimpleTextPairDataset(filtered_pairs, tokenizer, max_len=10)\n",
    "    \n",
    "    #exemple paire\n",
    "    src, tgt = dataset[0]\n",
    "    print(\"Source tensor:\", src)\n",
    "    print(\"Target tensor:\", tgt)\n",
    "\n",
    "    #traduction origninale ex\n",
    "    src_decoded = tokenizer.decode(src.tolist())\n",
    "    tgt_decoded = tokenizer.decode(tgt.tolist())\n",
    "    print(\"Source (decoded):\", src_decoded)\n",
    "    print(\"Target (decoded):\", tgt_decoded)\n",
    "\n",
    "    vocab_size = tokenizer.vocab_size()\n",
    "    model = Seq2SeqRNN(vocab_size=vocab_size, embed_dim=128, hidden_dim=256).to(device)\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    # j'aurais pu faire train, test, split \n",
    "    train_model(model, dataset, batch_size=1, epochs=10)\n",
    "    \n",
    "    # Test\n",
    "    test_en = \"hello\"\n",
    "    translation = translate_sentence_debug(model, test_en, tokenizer)\n",
    "    print(f\"\\nTraduction de '{test_en}' = '{translation}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044143a-e13e-4e6c-98ec-26a5dd63598e",
   "metadata": {},
   "source": [
    "#### Partie 2 Tokenizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6786877b-77ec-4388-af87-7f3b64c5a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#PARTIE 2 : SENTENCEPIECE TOKENIZERS\n",
    "\n",
    "#Créer un sous-ensemble du dataset pour l'entraînement des tokenizers\n",
    "subset_size = 10000\n",
    "df_subset = df.sample(n=subset_size, random_state=42)  \n",
    "df_subset[\"en\"].to_csv(\"en_sub.txt\", index=False, header=False)\n",
    "df_subset[\"fr\"].to_csv(\"fr_sub.txt\", index=False, header=False)\n",
    "\n",
    "#Entraînement du tokenizer anglais\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='en_sub.txt',\n",
    "    model_prefix='spm_en',\n",
    "    vocab_size=5000, \n",
    "    model_type='bpe', #BPE plutôt que le default cf. compte-rendu\n",
    "    user_defined_symbols=['<pad>','<bos>', '<eos>']\n",
    ")\n",
    "#Entraînement du tokenizer français\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='fr_sub.txt',\n",
    "    model_prefix='spm_fr',\n",
    "    vocab_size=5000,\n",
    "    model_type='bpe',\n",
    "    user_defined_symbols=['<pad>','<bos>', '<eos>']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "898178a6-337a-4bc9-abd5-668e68c18fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer SentencePiece\n",
    "\n",
    "class SentencePieceTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenizer utilisant SentencePiece.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_path):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=model_path)\n",
    "        self.vocab_size = self.sp.get_piece_size()\n",
    "        \n",
    "        # IDs pour tokens spéciaux\n",
    "        self.pad_id = self.sp.piece_to_id(\"<pad>\")\n",
    "        self.bos_id = self.sp.piece_to_id(\"<bos>\")\n",
    "        self.eos_id = self.sp.piece_to_id(\"<eos>\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode une phrase en ID via SentencePiece.\n",
    "        \"\"\"\n",
    "        return [self.bos_id] + self.sp.encode(text, out_type=int) + [self.eos_id]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        \"\"\"\n",
    "        Decode une liste d'ID en texte.\n",
    "        \"\"\"\n",
    "        tokens = [t for t in tokens if t != self.pad_id]\n",
    "        return self.sp.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d0ec4242-8e54-41d0-921d-6edc70e7c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset avec SentencePiece et filtrage par longueur max\n",
    "\n",
    "class SPTextPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset pour paires (source_en, target_fr) utilisant SentencePiece.\n",
    "    tronquage.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, sp_en, sp_fr, max_len=80):\n",
    "        self.sp_en = sp_en\n",
    "        self.sp_fr = sp_fr\n",
    "        self.max_len = max_len\n",
    "        self.pairs = []\n",
    "\n",
    "        for en_text, fr_text in pairs:\n",
    "            # retirer symboles bizarres (exemple)\n",
    "            en_text = re.sub(r\"[|~#]\", \"\", en_text)\n",
    "            fr_text = re.sub(r\"[|~#]\", \"\", fr_text)\n",
    "\n",
    "            src_ids = sp_en.encode(en_text)\n",
    "            tgt_ids = sp_fr.encode(fr_text)\n",
    "            \n",
    "            # Filtrer si la longueur dépasse max_len\n",
    "            if len(src_ids) <= max_len and len(tgt_ids) <= max_len:\n",
    "                self.pairs.append((src_ids, tgt_ids))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_ids, tgt_ids = self.pairs[idx]\n",
    "        src_tensor = torch.tensor(src_ids, dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor(tgt_ids, dtype=torch.long)\n",
    "        return src_tensor, tgt_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7902a764-c6a3-49ca-b631-ef1f264d4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modèle Seq2Seq RNN un peu plus complexe avec plus de fonctionnalités\n",
    "class Seq2SeqRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    RNN avec:\n",
    "      -deux embeddings (anglais / français)\n",
    "      -dropout\n",
    "      -un encodeur RNN + un décodeur RNN\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim=512, hidden_dim=1024, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Encoder RNN\n",
    "        self.encoder = nn.RNN(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=4,   \n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            nonlinearity='relu'\n",
    "        )\n",
    "        \n",
    "        # Decoder RNN\n",
    "        self.decoder = nn.RNN(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=4,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        # Projection linéaire\n",
    "        self.fc_out = nn.Linear(hidden_dim, tgt_vocab_size)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        src: [batch_size, src_len]\n",
    "        tgt: [batch_size, tgt_len]\n",
    "        \"\"\"\n",
    "        #Encodeur\n",
    "        emb_src = self.embedding_src(src)      #[B, src_len, embed_dim]\n",
    "        emb_src = self.dropout(emb_src)        #Dropout sur embeddings\n",
    "        _, hidden = self.encoder(emb_src)      #hidden => [num_layers, B, hidden_dim]\n",
    "        \n",
    "        #Décodeur (teacher forcing)\n",
    "        emb_tgt = self.embedding_tgt(tgt)      #[B, tgt_len, embed_dim]\n",
    "        emb_tgt = self.dropout(emb_tgt)\n",
    "        outputs, _ = self.decoder(emb_tgt, hidden)\n",
    "        \n",
    "        #projection\n",
    "        logits = self.fc_out(outputs)          #[B, tgt_len, tgt_vocab_size]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "81d8e35e-2562-4dbd-b07d-afd8f3f90110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Fonction collate_fn pour le padding\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: liste de tuples de tailles variables.\n",
    "    On va les pad pour obtenir [batch_size, max_seq_len].\n",
    "    \"\"\"\n",
    "    src_list, tgt_list = [], []\n",
    "    for (src, tgt) in batch:\n",
    "        src_list.append(src)\n",
    "        tgt_list.append(tgt)\n",
    "\n",
    "    src_padded = pad_sequence(src_list, batch_first=True, padding_value=sp_en.pad_id)  \n",
    "    tgt_padded = pad_sequence(tgt_list, batch_first=True, padding_value=sp_fr.pad_id)\n",
    "\n",
    "    return src_padded, tgt_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cc77e148-25bb-458d-b2e5-b687f0e38fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Boucle d'entraînement\n",
    "def train_one_epoch(model, dataloader, optimizer, device, pad_id):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    \n",
    "    for (src, tgt) in dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        # Décalage pour teacher forcing\n",
    "        tgt_input = tgt[:, :-1]   # tout sauf le dernier\n",
    "        tgt_target = tgt[:, 1:]   # tout sauf le premier\n",
    "        \n",
    "        logits = model(src, tgt_input)  # [B, seq_len, vocab_size]\n",
    "        \n",
    "        # On reshape pour la cross entropy\n",
    "        B, seq_len, vocab_size = logits.shape\n",
    "        logits_2d = logits.reshape(-1, vocab_size)   # [B*seq_len, vocab_size]\n",
    "        targets_1d = tgt_target.reshape(-1)          # [B*seq_len]\n",
    "\n",
    "         # Vérifie les données\n",
    "        assert torch.isfinite(src).all(), \"NaN ou inf dans les entrées source\"\n",
    "        assert torch.isfinite(tgt_input).all(), \"NaN ou inf dans les entrées cible\"\n",
    "        assert torch.isfinite(logits).all(), \"NaN ou inf dans les logits\"\n",
    "\n",
    "        loss = criterion(logits_2d, targets_1d)\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print(\"NaN détectée dans la perte. Vérifie les entrées et les logits.\")\n",
    "            print(f\"src: {src}\")\n",
    "            print(f\"tgt_input: {tgt_input}\")\n",
    "            print(f\"logits: {logits}\")\n",
    "            break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #gradient clipping\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calcul de la top-1 accuracy\n",
    "        _, preds_top1 = torch.max(logits_2d, dim=1)  \n",
    "        correct_top1 += (preds_top1 == targets_1d).sum().item()\n",
    "        \n",
    "        # Calcul de la top-5 accuracy\n",
    "        top5_vals, top5_idxs = torch.topk(logits_2d, k=5, dim=1)  \n",
    "        match_matrix = (top5_idxs == targets_1d.unsqueeze(1))\n",
    "        correct_top5 += match_matrix.any(dim=1).sum().item()\n",
    "        \n",
    "        total_samples += targets_1d.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    top1_acc = correct_top1 / total_samples\n",
    "    top5_acc = correct_top5 / total_samples\n",
    "    \n",
    "    return avg_loss, top1_acc, top5_acc\n",
    "\n",
    "def train_model(model, train_dataset, batch_size=16, epochs=30):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=my_collate_fn\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        avg_loss, top1, top5 = train_one_epoch(model, dataloader, optimizer, device, sp_fr.pad_id)\n",
    "        print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f} - Top1: {top1:.4f} - Top5: {top5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "684459e7-528c-4de3-81b2-09ccfc2b4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fonction de traduction (décodage)\n",
    "def translate_sentence_debug(model, source_sentence, sp_en, sp_fr, max_len=80, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Traduction en->fr:\n",
    "      -encode la phrase source\n",
    "      -génère token par token\n",
    "      -décode le résultat en string\n",
    "      -sampling\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Encode la phrase source\n",
    "    src_ids = sp_en.encode(source_sentence)\n",
    "    src_tensor = torch.tensor([src_ids], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encodeur\n",
    "        emb_src = model.embedding_src(src_tensor)\n",
    "        emb_src = model.dropout(emb_src)\n",
    "        _, hidden = model.encoder(emb_src)  #[1, 1, hidden_dim]\n",
    "    \n",
    "    generated_ids = []\n",
    "    bos_id = sp_fr.bos_id\n",
    "    eos_id = sp_fr.eos_id\n",
    "    \n",
    "    # On initie le décodeur avec <bos>\n",
    "    current_token = torch.tensor([[bos_id]], dtype=torch.long, device=device)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            emb_tgt = model.embedding_tgt(current_token)\n",
    "            emb_tgt = model.dropout(emb_tgt)\n",
    "            output, hidden = model.decoder(emb_tgt, hidden)  # [1, 1, hidden_dim]\n",
    "            logits = model.fc_out(output.squeeze(1))         # [1, vocab_size]\n",
    "            \n",
    "            # sampling\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "          \n",
    "            \n",
    "            if next_token_id == eos_id:\n",
    "                break\n",
    "            generated_ids.append(next_token_id)\n",
    "            \n",
    "            current_token = torch.tensor([[next_token_id]], dtype=torch.long, device=device)\n",
    "    \n",
    "    # Décodage vers texte\n",
    "    translation = sp_fr.decode(generated_ids)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cbb350ff-f01f-4db3-9a5c-3c9d0702627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_filter_dataset(df, tokenizer_en, tokenizer_fr, max_len=80, min_len=3, max_ratio=1.5):\n",
    "    # Supprimer valeurs vides\n",
    "    df.dropna(subset=[\"en\", \"fr\"], inplace=True)\n",
    "    df = df[(df[\"en\"].str.strip() != \"\") & (df[\"fr\"].str.strip() != \"\")]\n",
    "    \n",
    "    # Supprimer les doublons\n",
    "    df.drop_duplicates(subset=[\"en\", \"fr\"], inplace=True)\n",
    "    \n",
    "    # Nettoyer le texte\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r\"[|~#]\", \"\", text)               # Retirer certains caractères\n",
    "        text = re.sub(r\"<[^>]+>\", \"\", text)             # Supprimer balises HTML\n",
    "        text = re.sub(r\"\\s+\", \" \", text)                # Espaces multiples\n",
    "        return text.strip()\n",
    "    \n",
    "    df[\"en\"] = df[\"en\"].apply(clean_text)\n",
    "    df[\"fr\"] = df[\"fr\"].apply(clean_text)\n",
    "    \n",
    "    # Filtrer par longueur\n",
    "    def filter_by_length(row):\n",
    "        en_len = len(tokenizer_en.encode(row[\"en\"]))\n",
    "        fr_len = len(tokenizer_fr.encode(row[\"fr\"]))\n",
    "        return min_len <= en_len <= max_len and min_len <= fr_len <= max_len\n",
    "    \n",
    "    df = df[df.apply(filter_by_length, axis=1)]\n",
    "    \n",
    "    # Filtrer les phrases déséquilibrées\n",
    "    def filter_unbalanced(row):\n",
    "        en_len = len(tokenizer_en.encode(row[\"en\"]))\n",
    "        fr_len = len(tokenizer_fr.encode(row[\"fr\"]))\n",
    "        ratio = max(en_len / fr_len, fr_len / en_len)\n",
    "        return ratio <= max_ratio\n",
    "    \n",
    "    df = df[df.apply(filter_unbalanced, axis=1)]\n",
    "    \n",
    "    # Réindexer le DataFrame\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3748d7-f7ca-46f8-beb1-9aad7e7096eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Appareil utilisé : cuda\n",
      "[INFO] Dataset initial chargé avec 100000 lignes.\n",
      "[INFO] Dataset nettoyé sauvegardé dans 'cleaned_dataset.csv'.\n",
      "[INFO] Nombre de lignes après nettoyage : 77793\n",
      "[INFO] Dataset nettoyé chargé avec 77793 lignes.\n",
      "[INFO] Premières lignes :                                                   en  \\\n",
      "0  Changing Lives Changing Society How It Works T...   \n",
      "1                                           Site map   \n",
      "2                                           Feedback   \n",
      "3                                            Credits   \n",
      "4                                           Français   \n",
      "\n",
      "                                                  fr  \n",
      "0  Il a transformé notre vie Il a transformé la s...  \n",
      "1                                       Plan du site  \n",
      "2                                        Rétroaction  \n",
      "3                                            Crédits  \n",
      "4                                            English  \n",
      "[INFO] Nombre de paires générées : 77295\n",
      "[INFO] Exemple d'une paire (anglais -> français) :\n",
      "   ('Changing Lives Changing Society How It Works Technology Drives Change Home Concepts Teachers Search Overview Credits HHCC Web Reference Feedback Virtual Museum of Canada Home Page', 'Il a transformé notre vie Il a transformé la société Son fonctionnement La technologie, moteur du changement Accueil Concepts Enseignants Recherche Aperçu Collaborateurs Web HHCC Ressources Commentaires Musée virtuel du Canada')\n",
      "[INFO] Vocab anglais : 5000 tokens.\n",
      "[INFO] Vocab français : 5000 tokens.\n",
      "[DEBUG] Vérification du token <pad> ID :\n",
      "PAD_ID anglais : 3\n",
      "PAD_ID français : 3\n",
      "[INFO] Nombre de paires après filtrage (max_len=80) : 77295\n",
      "[INFO] Modèle Seq2SeqRNN instancié :\n",
      "Seq2SeqRNN(\n",
      "  (embedding_src): Embedding(5000, 512)\n",
      "  (embedding_tgt): Embedding(5000, 512)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (encoder): RNN(512, 1024, num_layers=4, batch_first=True, dropout=0.2)\n",
      "  (decoder): RNN(512, 1024, num_layers=4, batch_first=True, dropout=0.2)\n",
      "  (fc_out): Linear(in_features=1024, out_features=5000, bias=True)\n",
      ")\n",
      "[INFO] Démarrage de l'entraînement...\n",
      "Epoch 1/30 - Loss: 5.2716 - Top1: 0.0772 - Top5: 0.1573\n",
      "Epoch 2/30 - Loss: 4.4398 - Top1: 0.1093 - Top5: 0.2083\n",
      "Epoch 3/30 - Loss: 4.0846 - Top1: 0.1297 - Top5: 0.2356\n",
      "Epoch 4/30 - Loss: 3.8586 - Top1: 0.1435 - Top5: 0.2527\n",
      "Epoch 5/30 - Loss: 3.7036 - Top1: 0.1535 - Top5: 0.2645\n",
      "Epoch 6/30 - Loss: 3.5854 - Top1: 0.1610 - Top5: 0.2734\n",
      "Epoch 7/30 - Loss: 3.4926 - Top1: 0.1667 - Top5: 0.2798\n",
      "Epoch 8/30 - Loss: 3.4160 - Top1: 0.1715 - Top5: 0.2853\n",
      "Epoch 9/30 - Loss: 3.3504 - Top1: 0.1759 - Top5: 0.2905\n",
      "Epoch 10/30 - Loss: 3.2941 - Top1: 0.1793 - Top5: 0.2944\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#7) Script principal\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Appareil utilisé : {device}\")\n",
    "\n",
    "    dataset_path = \"Desktop/Projet_Methodes_Apprentissages/inputs/en-fr.csv\"\n",
    "\n",
    "    #Charger les données initiales\n",
    "    df_init = pd.read_csv(dataset_path, usecols=[\"en\", \"fr\"], nrows=100000)\n",
    "    print(f\"[INFO] Dataset initial chargé avec {len(df_init)} lignes.\")\n",
    "\n",
    "    #Nettoyer et filtrer les données\n",
    "    sp_en = SentencePieceTokenizer(\"spm_en.model\")\n",
    "    sp_fr = SentencePieceTokenizer(\"spm_fr.model\")\n",
    "    cleaned_df = clean_and_filter_dataset(df_init, tokenizer_en=sp_en, tokenizer_fr=sp_fr, max_len=80)\n",
    "    cleaned_df.to_csv(\"cleaned_dataset.csv\", index=False)\n",
    "    print(f\"[INFO] Dataset nettoyé sauvegardé dans 'cleaned_dataset.csv'.\")\n",
    "    print(f\"[INFO] Nombre de lignes après nettoyage : {len(cleaned_df)}\")\n",
    "    \n",
    "    #Charger le dataset nettoyé\n",
    "    df = pd.read_csv(\"cleaned_dataset.csv\", usecols=[\"en\", \"fr\"])\n",
    "    print(f\"[INFO] Dataset nettoyé chargé avec {len(df)} lignes.\")\n",
    "    print(f\"[INFO] Premières lignes : {df.head()}\")\n",
    "\n",
    "    #Créer les paires (anglais, français)\n",
    "    pairs = []\n",
    "    for en_text, fr_text in zip(df[\"en\"], df[\"fr\"]):\n",
    "        if isinstance(en_text, str) and isinstance(fr_text, str) and len(en_text) > 3 and len(fr_text) > 3:\n",
    "            pairs.append((en_text, fr_text))\n",
    "\n",
    "    print(f\"[INFO] Nombre de paires générées : {len(pairs)}\")\n",
    "    print(\"[INFO] Exemple d'une paire (anglais -> français) :\")\n",
    "    print(\"  \", pairs[0])\n",
    "\n",
    "    #Charger les tokenizers\n",
    "    sp_en = SentencePieceTokenizer(\"spm_en.model\")\n",
    "    sp_fr = SentencePieceTokenizer(\"spm_fr.model\")\n",
    "    print(f\"[INFO] Vocab anglais : {sp_en.vocab_size} tokens.\")\n",
    "    print(f\"[INFO] Vocab français : {sp_fr.vocab_size} tokens.\")\n",
    "    print(\"[DEBUG] Vérification du token <pad> ID :\")\n",
    "    print(f\"PAD_ID anglais : {sp_en.pad_id}\")\n",
    "    print(f\"PAD_ID français : {sp_fr.pad_id}\")\n",
    "\n",
    "\n",
    "    #Créer le dataset avec filtrage de longueur max\n",
    "    sp_dataset = SPTextPairDataset(pairs, sp_en, sp_fr, max_len=80)\n",
    "    print(f\"[INFO] Nombre de paires après filtrage (max_len=80) : {len(sp_dataset)}\")\n",
    "\n",
    "    #Instancier le modèle Seq2Seq RNN\n",
    "    model = Seq2SeqRNN(\n",
    "        src_vocab_size=sp_en.vocab_size, \n",
    "        tgt_vocab_size=sp_fr.vocab_size,\n",
    "        embed_dim=512,   \n",
    "        hidden_dim=1024,  \n",
    "        dropout=0.2      \n",
    "    )\n",
    "    print(\"[INFO] Modèle Seq2SeqRNN instancié :\")\n",
    "    print(model)\n",
    "\n",
    "    #Entraîner le modèle\n",
    "    print(\"[INFO] Démarrage de l'entraînement...\")\n",
    "    train_model(model, sp_dataset, batch_size=16, epochs=30)\n",
    "    print(\"[INFO] Entraînement terminé.\")\n",
    "\n",
    "    #Tester le modèle avec une traduction\n",
    "    test_en = \"Hello world, how are you?\"\n",
    "    translation = translate_sentence_debug(model, test_en, sp_en, sp_fr, max_len=80, device=device)\n",
    "    print(f\"\\n[INFO] Traduction de '{test_en}' = '{translation}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d45138-20c6-410d-825a-b9235a447bb2",
   "metadata": {},
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "220f296d-1c8b-468e-bc20-b0bd48477c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TRANSFORMER\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float, max_len: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size: int):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, emb_size, nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=2048, dropout=0.2):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_size,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        outs = self.transformer(\n",
    "            src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "            src_padding_mask, tgt_padding_mask, memory_key_padding_mask\n",
    "        )\n",
    "        return self.generator(outs)\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask.type(torch.bool)  # Transforme en masque booléen\n",
    "\n",
    "def create_mask(src, tgt, device):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len).type(torch.bool)  \n",
    "\n",
    "    src_padding_mask = (src == sp_en.pad_id).to(torch.bool)\n",
    "    tgt_padding_mask = (tgt == sp_fr.pad_id).to(torch.bool)\n",
    "\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=device).type(torch.bool)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db51c6e9-c5cd-4a9e-aa89-bd7fc282f494",
   "metadata": {},
   "source": [
    "#### wandb pour le suivi des runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "59ae0972-247b-4ab9-bc2f-0cc37327e4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: marius-casamian (marius-casamian-sophia-antipolis). Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f20741aa94245fe838690380a7a8dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Marius\\wandb\\run-20250112_161357-59xjyrvb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marius-casamian-sophia-antipolis/transformer-pytorch-traduction/runs/59xjyrvb' target=\"_blank\">visionary-monkey-4</a></strong> to <a href='https://wandb.ai/marius-casamian-sophia-antipolis/transformer-pytorch-traduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marius-casamian-sophia-antipolis/transformer-pytorch-traduction' target=\"_blank\">https://wandb.ai/marius-casamian-sophia-antipolis/transformer-pytorch-traduction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marius-casamian-sophia-antipolis/transformer-pytorch-traduction/runs/59xjyrvb' target=\"_blank\">https://wandb.ai/marius-casamian-sophia-antipolis/transformer-pytorch-traduction/runs/59xjyrvb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialisation wandb\n",
    "wandb.init(\n",
    "    project=\"transformer-pytorch-traduction\",\n",
    "    entity=\"marius-casamian-sophia-antipolis\",\n",
    "    config={\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\":  50,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"embedding_size\": 512,\n",
    "        \"num_encoder_layers\": 6,\n",
    "        \"num_decoder_layers\": 6,\n",
    "    }\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9c6fb04f-fb4b-457a-8ecd-ce3e8407b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENTRAÎNEMENT & ÉVALUATION\n",
    "\n",
    "def train_epoch_transformer(model, optimizer, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_out = tgt[:, 1:]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, device)\n",
    "\n",
    "        logits = model(\n",
    "            src,\n",
    "            tgt_input,\n",
    "            src_mask.type(torch.bool),  # S'assurer que le type est cohérent\n",
    "            tgt_mask.type(torch.bool),  # Transformer float en bool si nécessaire\n",
    "            src_padding_mask.type(torch.bool),  # Idem pour le padding\n",
    "            tgt_padding_mask.type(torch.bool),\n",
    "            src_padding_mask.type(torch.bool)\n",
    "        )\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), tgt_out.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fbae19a0-33b1-44d1-a851-fc62d194833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71,806,544 total parameters.\n",
      "71,806,544 training parameters.\n",
      "Seq2SeqTransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout2): Dropout(p=0.2, inplace=False)\n",
      "          (dropout3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (generator): Linear(in_features=512, out_features=18000, bias=True)\n",
      "  (src_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(18000, 512)\n",
      "  )\n",
      "  (tgt_tok_emb): TokenEmbedding(\n",
      "    (embedding): Embedding(18000, 512)\n",
      "  )\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch 1: Train Loss = 6.0148\n",
      "Epoch 2: Train Loss = 5.1290\n",
      "Epoch 3: Train Loss = 4.7769\n",
      "Epoch 4: Train Loss = 4.5347\n",
      "Epoch 5: Train Loss = 4.3412\n",
      "Epoch 6: Train Loss = 4.1760\n",
      "Epoch 7: Train Loss = 4.0310\n",
      "Epoch 8: Train Loss = 3.9015\n",
      "Epoch 9: Train Loss = 3.7825\n",
      "Epoch 10: Train Loss = 3.6729\n",
      "Epoch 11: Train Loss = 3.5726\n",
      "Epoch 12: Train Loss = 3.4738\n",
      "Epoch 13: Train Loss = 3.3842\n",
      "Epoch 14: Train Loss = 3.3012\n",
      "Epoch 15: Train Loss = 3.2200\n",
      "Epoch 16: Train Loss = 3.1383\n",
      "Epoch 17: Train Loss = 3.0677\n",
      "Epoch 18: Train Loss = 2.9938\n",
      "Epoch 19: Train Loss = 2.9246\n",
      "Epoch 20: Train Loss = 2.8575\n",
      "Epoch 21: Train Loss = 2.7923\n",
      "Epoch 22: Train Loss = 2.7271\n",
      "Epoch 23: Train Loss = 2.6641\n",
      "Epoch 24: Train Loss = 2.6023\n",
      "Epoch 25: Train Loss = 2.5426\n",
      "Epoch 26: Train Loss = 2.4807\n",
      "Epoch 27: Train Loss = 2.4243\n",
      "Epoch 28: Train Loss = 2.3667\n",
      "Epoch 29: Train Loss = 2.3099\n",
      "Epoch 30: Train Loss = 2.2570\n",
      "Epoch 31: Train Loss = 2.2051\n",
      "Epoch 32: Train Loss = 2.1483\n",
      "Epoch 33: Train Loss = 2.0985\n",
      "Epoch 34: Train Loss = 2.0484\n",
      "Epoch 35: Train Loss = 1.9976\n",
      "Epoch 36: Train Loss = 1.9517\n",
      "Epoch 37: Train Loss = 1.9054\n",
      "Epoch 38: Train Loss = 1.8623\n",
      "Epoch 39: Train Loss = 1.8199\n",
      "Epoch 40: Train Loss = 1.7795\n",
      "Epoch 41: Train Loss = 1.7390\n",
      "Epoch 42: Train Loss = 1.7005\n",
      "Epoch 43: Train Loss = 1.6605\n",
      "Epoch 44: Train Loss = 1.6213\n",
      "Epoch 45: Train Loss = 1.5808\n",
      "Epoch 46: Train Loss = 1.5458\n",
      "Epoch 47: Train Loss = 1.5084\n",
      "Epoch 48: Train Loss = 1.4721\n",
      "Epoch 49: Train Loss = 1.4374\n",
      "Epoch 50: Train Loss = 1.4019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SCRIPT PRINCIPAL\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Chargement des données nettoyées\n",
    "    df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "    pairs = [(row[\"en\"], row[\"fr\"]) for _, row in df.iterrows()]\n",
    "\n",
    "    sp_en = SentencePieceTokenizer(\"spm_en.model\")\n",
    "    sp_fr = SentencePieceTokenizer(\"spm_fr.model\")\n",
    "\n",
    "    dataset = SPTextPairDataset(pairs, sp_en, sp_fr, max_len=50)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, collate_fn=my_collate_fn, shuffle=True)\n",
    "\n",
    "    #instancier le modèle Transformer\n",
    "    transformer = Seq2SeqTransformer(\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        emb_size=512,\n",
    "        nhead=8,\n",
    "        src_vocab_size=sp_en.vocab_size,\n",
    "        tgt_vocab_size=sp_fr.vocab_size\n",
    "    ).to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in transformer.parameters())\n",
    "    print(f\"{total_params:,} total parameters.\")\n",
    "    total_trainable_params = sum(\n",
    "        p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "    print(f\"{total_trainable_params:,} training parameters.\")\n",
    "    print(transformer)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=sp_fr.pad_id)\n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    # Entraîner le modèle Transformer\n",
    "    for epoch in range(50):\n",
    "        train_loss = train_epoch_transformer(transformer, optimizer, loss_fn, dataloader, device)\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}\")\n",
    "\n",
    "    torch.save(transformer, 'model.pth')\n",
    "    wandb.save('model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b8c68-7e49-4690-bcd8-c182e7d4677f",
   "metadata": {},
   "source": [
    "#### top-p sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "59f1d872-38de-4ec9-aa55-833b0b27b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# top-p sampling\n",
    "\n",
    "def top_p_sampling(logits: torch.Tensor, p: float, temperature: float) -> int:\n",
    "    \"\"\"\n",
    "    1) On applique le temp en divisant les logits\n",
    "    2) On calcule softmax pour obtenir la distribution\n",
    "    3) On trie les tokens par probabilité décroissante\n",
    "    4) On tronque là où la somme cumulée des proba dépasse p\n",
    "    5) On renormalise et on échantillonne\n",
    "    \"\"\"\n",
    "    #ggestion de la température\n",
    "    logits = logits / temperature\n",
    "    \n",
    "    # Distribution de probabilité\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    #Tri décroissant\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    #Tronquer au seuil p\n",
    "    cutoff_idx = torch.searchsorted(cumulative_probs, p).item()\n",
    "    truncated_probs = sorted_probs[:cutoff_idx + 1]\n",
    "    truncated_indices = sorted_indices[:cutoff_idx + 1]\n",
    "    \n",
    "    #Renormaliser\n",
    "    truncated_probs = truncated_probs / truncated_probs.sum()\n",
    "    \n",
    "    # Échantillonnage\n",
    "    sampled_idx = torch.multinomial(truncated_probs, 1).item()\n",
    "    next_token_id = truncated_indices[sampled_idx].item()\n",
    "    return next_token_id\n",
    "\n",
    "\n",
    "#on tente de générer la traductionn avec sampling\n",
    "\n",
    "def generate_translation_top_p(\n",
    "    model: nn.Module, \n",
    "    src_sentence: str,\n",
    "    tokenizer_src: SentencePieceTokenizer,\n",
    "    tokenizer_tgt: SentencePieceTokenizer,\n",
    "    device: torch.device,\n",
    "    p: float = 0.85,\n",
    "    max_len: int = 50,\n",
    "    temperature: float = 1.0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Etapes :\n",
    "      1) On encode src_sentence en ID.\n",
    "      2) On fait tourner le modèle encodeur-décodeur token par token.\n",
    "      3) A chaque étape, on applique top-p sampling pour choisir le token suivant.\n",
    "      4) On arrête si on rencontre <eos> ou si on dépasse max_len.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    #Encode la phrase source pour la passer au modèle\n",
    "    src_ids = tokenizer_src.encode(src_sentence)\n",
    "\n",
    "    \n",
    "    src_tensor = torch.tensor([[tokenizer_src.bos_id] + src_ids + [tokenizer_src.eos_id]],\n",
    "                              device=device)\n",
    "\n",
    "    # avec <bos> côté cible pour séquence générée\n",
    "    generated = [tokenizer_tgt.bos_id]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            tgt_tensor = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "            \n",
    "            # Crée les masques (vous avez déjà cette fonction)\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
    "                src_tensor, tgt_tensor, device\n",
    "            )\n",
    "            \n",
    "            #obtenir les logits\n",
    "            logits = model(\n",
    "                src_tensor, \n",
    "                tgt_tensor,\n",
    "                src_mask,\n",
    "                tgt_mask,\n",
    "                src_padding_mask,\n",
    "                tgt_padding_mask,\n",
    "                src_padding_mask  # key_padding_mask du memory\n",
    "            )\n",
    "\n",
    "            #les logits du dernier token\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "\n",
    "            #top-p sampling\n",
    "            next_token_id = top_p_sampling(next_token_logits, p=p, temperature=temperature)\n",
    "\n",
    "            #Test de fin\n",
    "            if next_token_id == tokenizer_tgt.eos_id:\n",
    "                break\n",
    "\n",
    "            # Sinon on ajoute ce token à la séquence générée\n",
    "            generated.append(next_token_id)\n",
    "\n",
    "\n",
    "    #on supprime <bos>\n",
    "    generated_tokens = generated[1:]\n",
    "    translation = tokenizer_tgt.decode(generated_tokens)\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "647e981d-c6ec-40ce-9a39-123e0a774862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Tests de traduction (top-p sampling) :\n",
      "Anglais : Change the world\n",
      "Français (top-p) : Pôle\n",
      "--------------------------------------------------\n",
      "Anglais : Someone who live in my country\n",
      "Français (top-p) : Les noms d’information\n",
      "--------------------------------------------------\n",
      "Anglais : Near the sea\n",
      "Français (top-p) : la population\n",
      "--------------------------------------------------\n",
      "Anglais : World is difficult\n",
      "Français (top-p) : L'Inde est le deuxième pays importateur\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Exemple de test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_sentences = [\n",
    "        \"Change the world\",\n",
    "        \"Someone who live in my country\",\n",
    "        \"Near the sea\",\n",
    "        \"World is difficult\"\n",
    "    ]\n",
    "    \n",
    "    print(\"[INFO] Tests de traduction (top-p sampling) :\")\n",
    "    for sentence in test_sentences:\n",
    "        translation = generate_translation_top_p(\n",
    "            model=transformer,\n",
    "            src_sentence=sentence,\n",
    "            tokenizer_src=sp_en,\n",
    "            tokenizer_tgt=sp_fr,\n",
    "            device=device,\n",
    "            p=0.7,         \n",
    "            max_len=50,\n",
    "            temperature=1.0 #\n",
    "        )\n",
    "        print(f\"Anglais : {sentence}\")\n",
    "        print(f\"Français (top-p) : {translation}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979c548-cdf5-4c07-a41a-984960816785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe6b9e-768d-42dd-b431-791adc07221d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
